{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e5a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5132e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the credit card fraud detection dataset from Kaggle\n",
    "# Dataset URL:\n",
    "# https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/data\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "credit_data = pd.read_csv('creditcard.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9045db87",
   "metadata": {},
   "source": [
    "# Requirements – Statistics \n",
    "\n",
    "## Exploration of data:\n",
    " \n",
    "1. Description of the data. This should include, brief discussion of the data and variables of interest, \n",
    "identification of outliers/missing data, and treatment of any outliers/missing data.  \n",
    "\n",
    "\n",
    "Descriptive Statistics - generate and analyse basic appropriate statistics for all variables - including \n",
    "measures of central tendency and spread and correlation.  (15 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc6b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info about the dataset\n",
    "print(\"Dataset shape:\", credit_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f404a4",
   "metadata": {},
   "source": [
    "Looking at the dataset there is 284807 rows and 31 collumns. what this means is there are 284807 credit card trasactions in this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5dd0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What type of collum are in this dataset\n",
    "print(\"Column types:\")\n",
    "print(credit_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ace63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first 5 rows of the dataset\n",
    "print(\"First few rows:\")\n",
    "print(credit_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a4385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset basic info\n",
    "print(\"Baisc info about the dataset:\")\n",
    "print(credit_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c64c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = credit_data.isnull().sum()\n",
    "total_missing = missing_values.sum()\n",
    "print(missing_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68fc204",
   "metadata": {},
   "source": [
    "### Checking for missing values\n",
    "\n",
    "no missing data found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b5e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "print(f\"Number of duplicate rows: {credit_data.duplicated().sum()}\")\n",
    "\n",
    "# Remove duplicate rows\n",
    "credit_card_transactions = len(credit_data)\n",
    "credit_data = credit_data.drop_duplicates()\n",
    "final_count = len(credit_data)\n",
    "\n",
    "print(f\"Duplicate rows removed: {credit_card_transactions - final_count}\")\n",
    "print(f\"Remaining rows after removing duplicates: {final_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a89c4f7",
   "metadata": {},
   "source": [
    "- Refrence to PCA how its done\n",
    "    - https://builtin.com/data-science/step-step-explanation-principal-component-analysis#:~:text=Principal%20Component%20Analysis%20(PCA):%20A%20Step%2Dby%2DStep,What%20Is%20Principal%20Component%20Analysis?%5Cn\n",
    "    \n",
    "    \n",
    "    ### Data description whats going on?\n",
    "    \n",
    "    The dataset has 284,807 credit card transations which has been recorded on a span of 2 days, the data was collected from eurapean card holders.\n",
    "    \n",
    "    #### The data consists of:\n",
    "    1. 284,807 rows which are individual credit card transactions.\n",
    "    2. 31 collumns.\n",
    "    3. Time frame when the transaction was done roughy 48 hours \n",
    "    \n",
    "    #### Variables what they mean\n",
    "    1. Time - float\n",
    "        - shows us the seconds per transaction\n",
    "    \n",
    "    2. v1 to v28 - float\n",
    "        - Princaple component analysis (PCA).\n",
    "        - ordered by most important to least important starting at v1.\n",
    "        - Taken from original dataset but then refind in a way to preserve data so it complies with GDPR.\n",
    "    \n",
    "    3. Amount - float\n",
    "        - Transactions in euros\n",
    "    \n",
    "    4. Class - int\n",
    "        - Show us what data is fradualent and what isn't \n",
    "        - if the class says 1 this means the data was fraudulent if it was a 0 then it was a normal transation.\n",
    "    \n",
    "    #### Principle components analysis (PCA) \n",
    "    \n",
    "    From v1 to v28 shows the PCA transacetions which is from the original datase.\n",
    "    \n",
    "    - Original data from the bank (Hypothetical to banking standards this is an example because no spacifics are mentioned into the dataset due to GDPR) \n",
    "    1. Merchant category code\n",
    "    2. Location (GPS coordinates)\n",
    "    3. Transaction time of day\n",
    "    4. Cardholder spending history\n",
    "    5. Device fingerprint\n",
    "    6. IP address patterns\n",
    "    7. Browser information\n",
    "    8. And many more sensitive details...\n",
    "    \n",
    "    - PCA transformation how its done:\n",
    "    1. Standardization: Original data scaled to zero mean, unit variance\n",
    "    2. Covariance Analysis: Checking to see if there is a relationship with the mean\n",
    "    3. Eigen-Decomposition: Linear algebra concept calculating the eigenvectors and eigenvalues  from the covariance matrix of the data\n",
    "        - Eigenvectors: shows the direction of max varients.\n",
    "        - Eigenvalues: Shows the importance of each direction.\n",
    "    4. Dimensionality Reduction: Retention of 28 most informative components\n",
    "    5. Anonymization: Irreversible transformation protecting sensitive details\n",
    "    \n",
    "    Personal note this was complecated to get my head around thankfully the dataset was already done this part when analysing it.\n",
    "    \n",
    "    ### What the PCA does it transforms all the bank statistics into:\n",
    "    \n",
    "    - the v1 to v28 lists the transactions by order. for example:\n",
    "        - V1 represents the most importat pattern like normal day going shopping for example\n",
    "        - V2 is the second most important pattern not related to v1 for example this could be international transaction and domestic ones.\n",
    "    \n",
    "    from V3 to V 28 this is the pattern this dataset follows \n",
    "\n",
    "    ### The assesment of the data\n",
    "    - no missing values were found in the dataset.\n",
    "    - There was  1,081 duplicate rows in the dataset which was removed.\n",
    "    - The data I didnt need to change there types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33e4b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate normal and fraudulent transactions\n",
    "normal_amounts = credit_data[credit_data['Class'] == 0]['Amount']\n",
    "fraud_amounts = credit_data[credit_data['Class'] == 1]['Amount']\n",
    "\n",
    "# Outlier detection using IQR method\n",
    "Q1 = credit_data['Amount'].quantile(0.25)\n",
    "Q3 = credit_data['Amount'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outlier_threshold = Q3 + 1.5 * IQR\n",
    "\n",
    "# Boxplot to visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([normal_amounts[normal_amounts < 500], fraud_amounts[fraud_amounts < 500]], labels=['Normal', 'Fraud'])\n",
    "plt.ylabel('Amount (€)')\n",
    "plt.title('Transaction Amount Distribution (Outliers Visible)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46795a1a",
   "metadata": {},
   "source": [
    "### Outliers using IQR\n",
    "- I used the inter quartile range to identify outliers but I didn't remove them from the dataset because the outliers in a credit card fraud dataset could be the fraud transactions were looking for therefore the values werent removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd69aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fraud vs non-fraud comparison:\")\n",
    "\n",
    "# Counts\n",
    "fraud_count = credit_data['Class'].sum()\n",
    "normal_count = len(credit_data) - fraud_count\n",
    "print(f\"Normal transactions: {normal_count:,} ({normal_count/len(credit_data)*100:.2f}%)\")\n",
    "print(f\"Fraud transactions: {fraud_count:} ({fraud_count/len(credit_data)*100:.2f}%)\")\n",
    "\n",
    "# Amount comparison\n",
    "print(f\"Transaction amount\")\n",
    "print(\"Normal transactions:\")\n",
    "print(f\"  Mean: €{credit_data[credit_data['Class']==0]['Amount'].mean():.2f}\")\n",
    "print(f\"  Median: €{credit_data[credit_data['Class']==0]['Amount'].median():.2f}\")\n",
    "print(f\"  Std: €{credit_data[credit_data['Class']==0]['Amount'].std():.2f}\")\n",
    "\n",
    "print(\"\\nFraud transactions:\")\n",
    "print(f\"  Mean: €{credit_data[credit_data['Class']==1]['Amount'].mean():.2f}\")\n",
    "print(f\"  Median: €{credit_data[credit_data['Class']==1]['Amount'].median():.2f}\")\n",
    "print(f\"  Std: €{credit_data[credit_data['Class']==1]['Amount'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4a3ca6",
   "metadata": {},
   "source": [
    "# UNIVARIATE PLOTS \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e86544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Univariate plots\n",
    "print(\"Univariate visualizations\")\n",
    "print(\"(Distribution of single variables)\")\n",
    "\n",
    "# Ensure the amount series are defined (in case this cell runs before other cells)\n",
    "normal_amounts = credit_data[credit_data['Class']==0]['Amount']\n",
    "fraud_amounts = credit_data[credit_data['Class']==1]['Amount']\n",
    "\n",
    "# Create figure for univariate plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Normal Amounts Histogram\n",
    "normal_filtered = normal_amounts[normal_amounts < 200]\n",
    "axes[0,0].hist(normal_filtered, bins=30, color='lightblue', edgecolor='black', alpha=0.7)\n",
    "axes[0,0].set_title('Normal Amounts Histogram (Under €200)')\n",
    "axes[0,0].set_xlabel('Amount (€)')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Fraud amounts histogram\n",
    "fraud_filtered = fraud_amounts[fraud_amounts < 200]\n",
    "axes[0,1].hist(fraud_filtered, bins=20, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "axes[0,1].set_title('Fraud Amounts Histogram (Under €200)')\n",
    "axes[0,1].set_xlabel('Amount (€)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# normal time distribution\n",
    "normal_hours = (credit_data[credit_data['Class']==0]['Time'] / 3600) % 24\n",
    "# use axes[1,0] (bottom-left) for the third plot\n",
    "axes[1,0].hist(normal_hours, bins=24, color='lightblue', edgecolor='black', alpha=0.7)\n",
    "axes[1,0].set_title('Normal: Time of Day')\n",
    "axes[1,0].set_xlabel('Hour of Day')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "axes[1,0].set_xlim(0, 24)\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Fraud time distribution\n",
    "fraud_hours = (credit_data[credit_data['Class']==1]['Time'] / 3600) % 24\n",
    "# use axes[1,1] (bottom-right) for the fourth plot\n",
    "axes[1,1].hist(fraud_hours, bins=24, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "axes[1,1].set_title('Fraud: Time of Day')\n",
    "axes[1,1].set_xlabel('Hour of Day')\n",
    "axes[1,1].set_ylabel('Frequency')\n",
    "axes[1,1].set_xlim(0, 24)\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Univariate analysis: Distribution of Individual Variables', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95113baf",
   "metadata": {},
   "source": [
    "### Univariate visualizations\n",
    "\n",
    "- Normal transactions are in blue and fraud transactions are in red.\n",
    "\n",
    "- First 2 charts check to see the different purchasing patterns of fraudulent transactions and ligitamit one by taking the amount spent and how frequent it happens.\n",
    "\n",
    "- The second row shows what time did the credit cards be used in a 24 hour timeline we can see that\n",
    "    - Normal transsactions: starts around 8 am then till the evening night time 9 to 11 pm transaction stop people are going to sleep.\n",
    "    - Fraud transaction: We can see moderant activity in the early mornings around 1 till 2 then a major peek at 11 am. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9255fb2f",
   "metadata": {},
   "source": [
    "# Bivariate plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9d67c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics for normal vs fraud transactions\n",
    "normal_amounts = credit_data[credit_data['Class']==0]['Amount']\n",
    "fraud_amounts = credit_data[credit_data['Class']==1]['Amount']\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Amount Statistics Bar Chart\n",
    "x_pos = [0, 1, 2, 3]\n",
    "labels = ['Normal Mean', 'Fraud Mean', 'Normal Median', 'Fraud Median']\n",
    "values = [normal_amounts.mean(), fraud_amounts.mean(), \n",
    "          normal_amounts.median(), fraud_amounts.median()]\n",
    "colors = ['lightblue', 'lightcoral', 'lightblue', 'lightcoral']\n",
    "\n",
    "plt.bar(x_pos, values, color=colors, edgecolor='black')\n",
    "plt.title('Amount Statistics Comparison')\n",
    "plt.ylabel('Amount (€)')\n",
    "plt.xticks(x_pos, labels, rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4732f8",
   "metadata": {},
   "source": [
    "#### Bivariate plots\n",
    "\n",
    "- Making a chart with 2 groups I found out the mean for the normal tranactions and fraud transactions by amount of money withdrawn and did the same for the median.\n",
    "\n",
    "- We can see the the fraud mean is higher then the ligitamint transaction because they took a greater amount of stolen money then the normal transactions but the median says the fraud transaction are lower then the normal transaction.\n",
    "\n",
    "- What this tells me is that the fraudulent transactions happen in small amounts but they are consistantly pulling money which then accumiates a larger mean of the total money pulled when compaired to normal ligitamit transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd79790",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://matplotlib.org/stable/gallery/subplots_axes_and_figures/subplots_demo.html\n",
    "#https://matplotlib.org/stable/gallery/lines_bars_and_markers/scatter_with_legend.html\n",
    "\n",
    "print(\"All PCA components (V1 to V28) plotted against Amount for both normal and fraud transactions.\")\n",
    "\n",
    "# Create grid\n",
    "fig, axes = plt.subplots(4, 7, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Get V columns\n",
    "v_columns = [f'V{i}' for i in range(1, 29)]\n",
    "\n",
    "# Sample data\n",
    "sample_normal = credit_data[credit_data['Class']==0].sample(500)\n",
    "sample_fraud = credit_data[credit_data['Class']==1]\n",
    "\n",
    "# Create plots\n",
    "for i, v_col in enumerate(v_columns):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    ax.scatter(sample_normal[v_col], sample_normal['Amount'], \n",
    "               alpha=0.3, color='blue', s=10)\n",
    "    ax.scatter(sample_fraud[v_col], sample_fraud['Amount'], \n",
    "               alpha=0.7, color='red', s=15)\n",
    "    \n",
    "    ax.set_title(v_col)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e3f639",
   "metadata": {},
   "source": [
    "# 1v to 28v scatterplot\n",
    "\n",
    "These are all the PCA components when tranformed into a scatterplot the grid shows the fraud transactions as red dots and blue transactions are ligitament normal transactions, I used a sample size of 500 for the dataset.\n",
    "\n",
    "The 2 comparisons are transaction amount wiht PCA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce090d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 6 Fraud Predictors vs Amount\")\n",
    "print(\"(Most correlated PCA components with Fraud)\")\n",
    "\n",
    "# Get top 6 predictors based on correlation with Class\n",
    "correlations = credit_data.corr()['Class'].abs().sort_values(ascending=False)\n",
    "top_predictors = correlations[1:7].index.tolist()  # Exclude Class itself\n",
    "\n",
    "# Create samples for plotting\n",
    "sample_normal_all = credit_data[credit_data['Class']==0].sample(min(1000, len(credit_data[credit_data['Class']==0])))\n",
    "sample_fraud_all = credit_data[credit_data['Class']==1]\n",
    "\n",
    "# Create figure for top predictors\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, predictor in enumerate(top_predictors):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Calculate correlation coefficient\n",
    "    corr_coef = credit_data.corr()['Class'][predictor]\n",
    "    \n",
    "    # Plot with different colors for fraud/normal\n",
    "    ax.scatter(sample_normal_all[predictor], sample_normal_all['Amount'], alpha=0.4, color='blue', label='Normal', s=20)\n",
    "    ax.scatter(sample_fraud_all[predictor], sample_fraud_all['Amount'], alpha=0.8, color='red', label='Fraud', s=25)\n",
    "    \n",
    "    # Add correlation info\n",
    "    ax.text(0.05, 0.95, f'r = {corr_coef:.3f}', \n",
    "            transform=ax.transAxes, fontsize=10,\n",
    "            verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_title(f'{predictor}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel(f'{predictor} Value')\n",
    "    ax.set_ylabel('Amount (€)' if i in [0, 3] else '')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add legend to first subplot\n",
    "axes[0].legend(loc='upper right', fontsize=9)\n",
    "\n",
    "plt.suptitle('Top 6 Fraud Predictors: Relationship with Transaction Amount', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print correlation info\n",
    "print(\"Correlation coefficients for top predictors:\")\n",
    "for predictor in top_predictors:\n",
    "    corr_value = credit_data.corr()['Class'][predictor]\n",
    "    direction = \"Negative\" if corr_value < 0 else \"Positive\"\n",
    "    print(f\"{predictor}: {corr_value:.3f} ({direction} correlation)\")\n",
    "\n",
    "print(\"Interpretation: Negative correlation means lower values = higher fraud risk\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8fabac",
   "metadata": {},
   "source": [
    "### Top 6 Fraud Predictors vs Amount\n",
    "\n",
    "This is more refined then the previus scatter plot because it shows us which PCA component has the highest correlations with fraud transaction, the lower the number is the higher and better it is at predicting the transaction that is a fraud one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be82288",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Multivariate plots\n",
    "print(\"(Multiple variables analyzed together)\")\n",
    "\n",
    "# Create figure for multivariate plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Transaction Types Bar Chart\n",
    "axes[0].bar(['Normal', 'Fraud'], [normal_count, fraud_count], \n",
    "            color=['lightblue', 'lightcoral'], edgecolor='black')\n",
    "axes[0].set_title('Transaction Types (Count)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "\n",
    "# Correlation Heatmap\n",
    "\n",
    "# Select subset of variables for clarity\n",
    "corr_vars = ['V17', 'V14', 'V12', 'V10', 'V16','V3', 'Amount', 'Class']\n",
    "corr_matrix = credit_data[corr_vars].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "im = axes[1].imshow(corr_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "axes[1].set_title('Correlation Heatmap (Multivariate)')\n",
    "axes[1].set_xticks(range(len(corr_vars)))\n",
    "axes[1].set_yticks(range(len(corr_vars)))\n",
    "axes[1].set_xticklabels(corr_vars, rotation=45, ha='right')\n",
    "axes[1].set_yticklabels(corr_vars)\n",
    "plt.colorbar(im, ax=axes[1])\n",
    "\n",
    "# Add correlation values as text\n",
    "for i in range(len(corr_vars)):\n",
    "    for j in range(len(corr_vars)):\n",
    "        text = axes[1].text(j, i, f'{corr_matrix.iloc[i, j]:.2f}',\n",
    "                           ha=\"center\", va=\"center\", \n",
    "                           color=\"white\" if abs(corr_matrix.iloc[i, j]) > 0.5 else \"black\",\n",
    "                           fontsize=8)\n",
    "\n",
    "plt.suptitle('Multivariate analysis: Multiple Variables Together', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f5be56",
   "metadata": {},
   "source": [
    "# Multivariate heatmap\n",
    "\n",
    "- I used the PCA components with the highest correlation to fraud transactions, (V17, V14, V12) are the top 3 highest indecatior showing a moderate negative correlation with fraud this is a good thing the lower the value the better it is at prediction.\n",
    "\n",
    "- PCA components start at 0  the lower the value the higher risk of fraud to happen.\n",
    "\n",
    "- Amount: using the amount being spent alone we cant use it to detect fraud transaction thats why we need to look at the different values as well like time and amount because amount on its own as seen i the heatmap there isnt strong correation to predict fraud transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a158791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary statistics table\n",
    "\n",
    "print(\"summary statistics\")\n",
    "\n",
    "print(f\"{'Metric':} {'Normal':} {'Fraud':}\")\n",
    "print(f\"{'Count':} {normal_count:,} {fraud_count:,}\")\n",
    "print(f\"{'Percentage':} {normal_count/(normal_count+fraud_count)*100:.2f}% , {fraud_count/(normal_count+fraud_count)*100:.2f}%\")\n",
    "print(f\"{'Mean Amount':} €{normal_amounts.mean():.2f}, €{fraud_amounts.mean():.2f}\")\n",
    "print(f\"{'Median Amount':} €{normal_amounts.median():.2f}, €{fraud_amounts.median():.2f}\")\n",
    "print(f\"{'Std Deviation':} €{normal_amounts.std():.2f}, €{fraud_amounts.std():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13b6e43",
   "metadata": {},
   "source": [
    "### Summary \n",
    "\n",
    "- There is 283,253 normal transactions which is 99.83% of all transactions and 473 fraud transactions which is 0.17% of the dataset.\n",
    "- The mean for normal transactions are €88.41, and for the fraud transaction the mean is €123.87.\n",
    "- The median amount for normal transactions are €22.00, meanwhile for fraud transactions are €9.82.\n",
    "- For the standard diviation for normal amount is €250.38, and for fraud it is €260.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e53c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations with fraud\n",
    "correlations = credit_data.corr()['Class'].sort_values()\n",
    "print(\"Top 5 correlations with Fraud (Class):\")\n",
    "print( correlations.head())\n",
    "print(\"\\nBottom 5 correlations with Fraud (Class):\")\n",
    "print(correlations.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200b255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "print(\"simple linear regression\")\n",
    "print(\"Regression: V17 with Amount\")\n",
    "\n",
    "X_v17 = sm.add_constant(credit_data['V17'])\n",
    "model_v17 = sm.OLS(credit_data['Amount'], X_v17).fit()\n",
    "\n",
    "print(f\"Equation: Amount = {model_v17.params['const']:.2f} + {model_v17.params['V17']:.2f}*V17\")\n",
    "print(f\"V17 coefficient: €{model_v17.params['V17']:.2f} per unit\")\n",
    "print(f\"R-squared: {model_v17.rsquared:.4f} ({model_v17.rsquared*100:.2f}%)\")\n",
    "print(f\"P-value: {model_v17.pvalues['V17']:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65949cd8",
   "metadata": {},
   "source": [
    "# Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ffb695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "# https://www.geeksforgeeks.org/machine-learning/understanding-logistic-regression/\n",
    "\n",
    "# logistic regression\n",
    "\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Can we predict fraud using top PCA components?\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use top predictors from earlier correlation analysis\n",
    "X = credit_data[['V14', 'V17', 'V12', 'V10', 'V16']]\n",
    "y = credit_data['Class']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit logistic regression\n",
    "logreg = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(\"Model Coefficients:\")\n",
    "for feature, coef in zip(X.columns, logreg.coef_[0]):\n",
    "    print(f\"{feature}: {coef:.4f}\")\n",
    "print(f\"Intercept: {logreg.intercept_[0]:.4f}\")\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"Accuracy: {logreg.score(X_test, y_test):.3f}\")\n",
    "print(f\"Fraud detection rate: {(y_pred[y_test==1].sum() / y_test.sum()):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167ba557",
   "metadata": {},
   "source": [
    "# logistic regression\n",
    "\n",
    "- Predicts to see whethere a transaction is fradulent or not it does this by compairing the collumn called class if the value is 1 then its a fradulanet transaction if its a 0 the its a legitament one.\n",
    "\n",
    "- The top predictors that were found using correlation analysis was \n",
    "    - ('V14', 'V17', 'V12', 'V10', 'V16')\n",
    "\n",
    "- The model coefficient show the stronger the negetive number the better at predicting the chance for fraud risk to happen.\n",
    "The best result is v14 because it has the highest value for predicting fraud risk. \n",
    "\n",
    "1. V14: -0.8647\n",
    "2. V12: -0.7565\n",
    "3. V17: 0.2167\n",
    "4. V16: -0.2465\n",
    "5. V10: -0.1775\n",
    "\n",
    "- Once the log regression starts at 0 once it falls under into the negetive value there is a higher risk of fraud acctiviaty to happen. \n",
    "-   log regression: -2.3721\n",
    "\n",
    "\n",
    "- When analysing the data I found it to be 98% right when detecting all transactions and 84.3% of detecting all fraud transactions.\n",
    "\n",
    "#### What is the outcome:\n",
    "\n",
    "- The model is not perfect it detects the large majoraty of the fradulent transaction but not 100% of them it is missing 16% of fraud cases.\n",
    "- v 14 and V 12 are the best at predicting, these 2 are the most important to predict the outcomes in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c0d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-sample t-test\n",
    "\n",
    "print(\"Two-sample t-test\")\n",
    "print(\"Do fraud and normal transactions have different average amounts?\")\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "normal_amounts = credit_data[credit_data['Class']==0]['Amount']\n",
    "fraud_amounts = credit_data[credit_data['Class']==1]['Amount']\n",
    "\n",
    "# Independent t-test\n",
    "t_stat, p_value = stats.ttest_ind(normal_amounts, fraud_amounts, equal_var=False)\n",
    "\n",
    "print(\"Two-Sample T-Test Results:\")\n",
    "print(f\"Normal mean: €{normal_amounts.mean():.2f}\")\n",
    "print(f\"Fraud mean: €{fraud_amounts.mean():.2f}\")\n",
    "print(f\"T-statistic: {t_stat:.3f}\")\n",
    "print(f\"P-value: {p_value:.10f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Significant: Fraud and normal transactions have different average amounts\")\n",
    "    if fraud_amounts.mean() > normal_amounts.mean():\n",
    "        print(\"  Fraud transactions have higher average amounts\")\n",
    "    else:\n",
    "        print(\"  Fraud transactions have lower average amounts\")\n",
    "else:\n",
    "    print(\"Not significant: No difference in average amounts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57c0de1",
   "metadata": {},
   "source": [
    "# Two-sample t-test\n",
    "\n",
    "- This test shows whether fraudulent and normal transactions have different average amount and is there a difference in mean between each other.\n",
    "\n",
    "- I compaired the means and average of two groups uing the welch t-test.\n",
    "\n",
    "#### The result\n",
    "- The dataset has total of 284,807 transactions legitamet ones and fraud.\n",
    "    - 283,253 transactions for normal transaction with a mean of €88.41\n",
    "    - 473 fraud transactions and the mean that was found is €123.87\n",
    "\n",
    "- What this tells me is there is a €35.46 higher average for fraud transactions\n",
    "\n",
    "- T-statistic: -2.961\n",
    "    - the negetive value tells me that the normal mean for transactions are less then fraud mean transactions.\n",
    "\n",
    "- P-value: 0.0032170246 \n",
    "    - If the p value falls below 0.05 which it has 0.0032 this tells us we should reject the null hypothesis, the averages are different then one another and not just luck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/python/how-to-perform-a-one-way-anova-in-python/\n",
    "\n",
    "# Oneway ANOVA\n",
    "\n",
    "print(\"One way ANOVA\")\n",
    "print(\"Do different transaction amounts have different V1 values?\")\n",
    "\n",
    "# Create amount categories\n",
    "credit_data['Amount_Category'] = pd.cut(credit_data['Amount'], \n",
    "                                        bins=[0, 50, 200, 1000, float('inf')], \n",
    "                                        labels=['Small (<€50)', 'Medium (€50-200)', \n",
    "                                                'Large (€200-1000)', 'Very Large (>€1000)'])\n",
    "\n",
    "# One way ANOVA: Amount category to V1\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Filter out NaN categories\n",
    "data_for_anova = credit_data.dropna(subset=['Amount_Category'])\n",
    "\n",
    "# ANOVA model\n",
    "model = ols('V1 ~ C(Amount_Category)', data=data_for_anova).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(\"ANOVA Results:\")\n",
    "print(anova_table)\n",
    "\n",
    "print(\"Significant: Different amount categories have different V1 values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184342c6",
   "metadata": {},
   "source": [
    "# One way ANOVA\n",
    "- Im testing to see if different transaction amounts have a different v1 value, I did this by spliting the transactions into 4 groups:\n",
    "    - Small (<€50)\n",
    "    - Medium (€50-200) \n",
    "    - Large (€200-1000)\n",
    "    - Very Large (>€1000)\n",
    "\n",
    "- The result is that the f-statistic = 3630.845482\n",
    "    - This tells us that there is a huge difference in the groups when compaired to there value each group is very different then one another.\n",
    "\n",
    "- for the p-value its below 0.0001 this means that there is a little to no chance of this result to be random we can confidently say there is a 99.99% confidence that each group is different.\n",
    "\n",
    "Once we have our results we can say v1 is the primary PCA component and it shows us that small purchase have a very different v1 value then the large ones. This helps us find a pattern with the fraud transactions activity we can see in previus results that the fraud activities they take a small amounts consistantly that add up to be more then your average normal transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e16414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_1samp.html\n",
    "\n",
    "# One - sample t-test: Is average normal transaction amount €100?\n",
    "\n",
    "print(\"One - sample t-test\")\n",
    "print(\"Is the average normal transaction amount €100?\")\n",
    "\n",
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "# One-sample t-test\n",
    "test_value = 100  # Test if mean = €100\n",
    "t_stat, p_value = ttest_1samp(normal_amounts, popmean=test_value)\n",
    "\n",
    "print(\"One-Sample T-Test Results:\")\n",
    "\n",
    "print(f\"Sample mean: €{normal_amounts.mean():.2f}\")\n",
    "print(f\"Test value: €{test_value}\")\n",
    "print(f\"T-statistic: {t_stat:.3f}\")\n",
    "print(f\"P-value: {p_value:.6f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"Significant: Average normal transaction amount is NOT €{test_value}\")\n",
    "    if normal_amounts.mean() > test_value:\n",
    "        print(f\"  It is higher than €{test_value}\")\n",
    "    else:\n",
    "        print(f\"  It is lower than €{test_value}\")\n",
    "else:\n",
    "    print(f\"Not significant: Cannot reject that average = €{test_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51075ef2",
   "metadata": {},
   "source": [
    "# One - sample t-test\n",
    "\n",
    "- Im testing to see if there is an "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
